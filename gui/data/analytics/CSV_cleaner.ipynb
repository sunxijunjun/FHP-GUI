{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the notebook to the same folder as raw data. The filename is hard-coded for integrated_data now, so is the filtering settings via boolean logic in pandas. Check them before using it.\n",
    "\n",
    "What it does:\n",
    "\n",
    "1.Find the files with designated prefix in name.\n",
    "\n",
    "2.Keep interested columns while dropping all others.\n",
    "\n",
    "3.Drop rows with no value in interested values.\n",
    "\n",
    "4.Remove dublicate entries by timestamp(optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='data_cleaning.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame based on specified conditions.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to clean.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    columns_to_keep = ['timestamp','sensor_2', 'sensor_4', 'mv_1', 'mv_2', 'mv_3', 'mv_4', 'fhp', 'prediction','notes','bad_posture_command','model_threshold','model_notes']\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    condition = (\n",
    "        (df['sensor_2'].notna() & df['sensor_4'].notna()) |  # Both sensor_2 and sensor_4 have values\n",
    "        (df[['mv_1', 'mv_2', 'mv_3', 'mv_4']].notna().all(axis=1))  # All mv_1, mv_2, mv_3, mv_4 have values\n",
    "    )\n",
    "\n",
    "\n",
    "    cleaned_df = df[condition]\n",
    "    return cleaned_df\n",
    "\n",
    "def remove_duplicates(df):\n",
    "    \"\"\"\n",
    "    Removes duplicate rows based on the 'timestamp' column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame from which to remove duplicates.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    duplicates = df[df.duplicated(subset='timestamp', keep=False)]\n",
    "    \n",
    "    if not duplicates.empty:\n",
    "        print(\"\\nDuplicate rows:\")\n",
    "        print(duplicates)\n",
    "        logging.info(f\"Duplicate rows found:\\n{duplicates}\")\n",
    "        \n",
    "    df_before = df.shape[0]\n",
    "    df = df.drop_duplicates(subset='timestamp', keep='first')\n",
    "    df_after = df.shape[0]\n",
    "    logging.info(f\"Removed duplicates: {df_before - df_after} rows dropped.\")\n",
    "    return df\n",
    "\n",
    "def process_files(directory, remove_duplicates = True):\n",
    "    \"\"\"\n",
    "    Processes all CSV files in the specified directory. Remove duplicates as default option.\n",
    "    \n",
    "    Parameters:\n",
    "        directory (str): The directory containing the CSV files.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('integrated_data') and filename.endswith('.csv'):\n",
    "\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            original_row_count = df.shape[0]\n",
    "            logging.info(f\"Processing {filename}: Original row count: {original_row_count}\")\n",
    "\n",
    "            cleaned_df = clean_data(df)\n",
    "            cleaned_df = remove_duplicates(cleaned_df)\n",
    "\n",
    "            # Log the cleaned number of rows\n",
    "            cleaned_row_count = cleaned_df.shape[0]\n",
    "            logging.info(f\"After cleaning {filename}: Cleaned row count: {cleaned_row_count}\")\n",
    "\n",
    "            cleaned_filename = f\"cleaned_{filename}\"\n",
    "            cleaned_file_path = os.path.join(directory, cleaned_filename)\n",
    "            cleaned_df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "            logging.info(f\"Saved cleaned data to {cleaned_file_path}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './'  # Currently set to the same location\n",
    "process_files(directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
